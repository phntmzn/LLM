# Transfer Learning Configuration

general:
  seed: 42                          # Random seed for reproducibility
  device: "cuda"                    # Device for training ("cuda" or "cpu")
  num_epochs: 5                     # Number of fine-tuning epochs
  batch_size: 8                     # Batch size for training
  eval_batch_size: 8                # Batch size for evaluation
  max_sequence_length: 512          # Maximum sequence length
  gradient_clipping: 1.0            # Gradient clipping threshold

learning_rate:
  base_lr: 5e-5                     # Base learning rate for fine-tuning
  warmup_steps: 500                 # Number of warmup steps
  lr_decay: "linear"                # Learning rate decay schedule ("cosine", "linear", "none")
  min_lr: 1e-6                      # Minimum learning rate

optimizer:
  type: "adamw"                     # Optimizer type ("adamw", "sgd", etc.)
  weight_decay: 0.01                # Weight decay for regularization

scheduler:
  type: "linear"                    # Scheduler type ("linear", "cosine", etc.)
  num_training_steps: 20000         # Total training steps
  num_warmup_steps: 500             # Warmup steps for scheduler

data:
  train_data_path: "data/processed/fine_tune_train.json"  # Path to training data
  val_data_path: "data/processed/fine_tune_val.json"      # Path to validation data
  shuffle_data: true                                      # Whether to shuffle the data
  num_workers: 4                                          # Number of workers for data loading

model:
  pretrained_checkpoint: "checkpoints/gpt2_pretrained.pth"  # Path to the pretrained model checkpoint
  save_path: "checkpoints/gpt2_finetuned.pth"              # Path to save the fine-tuned model
  freeze_embeddings: true                                  # Freeze token and position embeddings
  freeze_layers: 10                                        # Number of layers to freeze from the bottom
  lora_rank: 4                                             # LoRA rank for lightweight fine-tuning
  tie_weights: true                                        # Tie input and output embeddings

evaluation:
  eval_frequency: 1                                        # Frequency (in epochs) for evaluation
  metrics: ["loss", "accuracy", "perplexity"]             # Metrics to track during evaluation

logging:
  log_frequency: 20                                        # Frequency (in steps) for logging
  log_dir: "results/logs/"                                 # Directory to save logs

checkpointing:
  save_frequency: 1                                        # Frequency (in epochs) to save checkpoints
  checkpoint_dir: "results/models/"                        # Directory to save model checkpoints
  resume_from_checkpoint: false                            # Whether to resume training from a checkpoint
  checkpoint_path: "results/models/gpt_transfer_checkpoint.pth"  # Path to load a checkpoint

augmentation:
  enable: false                                            # Enable or disable data augmentation
  strategies: []                                           # List of augmentation strategies (e.g., "token_masking", "shuffling")

debugging:
  enable: false                                            # Enable debugging mode (e.g., shorter epochs)
  debug_steps: 50                                          # Number of steps to run in debugging mode
