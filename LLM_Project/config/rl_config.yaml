# Reinforcement Learning Configuration

environment:
  name: "CustomEnv"               # Name of the environment (e.g., "CartPole-v1", "CustomEnv")
  max_steps: 1000                 # Maximum number of steps per episode
  reward_threshold: 200           # Reward threshold for considering the task solved
  seed: 42                        # Random seed for reproducibility

agent:
  policy_network:
    hidden_layers: [256, 256]     # Hidden layers for the policy network
    activation: "relu"            # Activation function (e.g., "relu", "tanh")
    dropout_rate: 0.2             # Dropout rate for regularization

  value_network:
    hidden_layers: [256, 256]     # Hidden layers for the value network
    activation: "relu"            # Activation function (e.g., "relu", "tanh")

  gamma: 0.99                     # Discount factor for future rewards
  learning_rate: 3e-4             # Learning rate for optimizer
  epsilon_start: 1.0              # Starting value of epsilon for exploration
  epsilon_end: 0.1                # Minimum value of epsilon
  epsilon_decay: 10000            # Number of steps to decay epsilon

training:
  num_episodes: 500               # Total number of episodes to train
  max_steps_per_episode: 1000     # Maximum number of steps per episode
  batch_size: 64                  # Batch size for experience replay
  buffer_size: 100000             # Replay buffer size
  update_frequency: 4             # Frequency (in steps) to update networks
  target_update_frequency: 100    # Frequency (in steps) to update the target network

optimizer:
  type: "adam"                    # Optimizer type ("adam", "sgd", etc.)
  weight_decay: 0.01              # Weight decay for regularization

logging:
  log_dir: "results/logs/"        # Directory to save training logs
  log_frequency: 10               # Frequency (in episodes) to log metrics
  save_model_frequency: 50        # Frequency (in episodes) to save model checkpoints

evaluation:
  num_episodes: 10                # Number of episodes to evaluate the agent
  render: false                   # Whether to render the environment during evaluation
  save_video: false               # Save evaluation episodes as videos

checkpointing:
  save_dir: "results/models/"     # Directory to save model checkpoints
  load_checkpoint: false          # Whether to load a previous checkpoint
  checkpoint_path: "results/models/rl_checkpoint.pth" # Path to load the checkpoint
